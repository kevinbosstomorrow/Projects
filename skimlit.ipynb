{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4a5903a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [........................................................................] 28908792 / 28908792"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'train.txt'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "#url = 'https://raw.githubusercontent.com/Franck-Dernoncourt/pubmed-rct/master/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt'\n",
    "#wget.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6890c0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,confusion_matrix,accuracy_score,precision_score,classification_report,recall_score\n",
    "pd.set_option(\"display.max_rows\", 20, \"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "434559e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints', 'dev.txt', 'skimlit.ipynb', 'test.txt', 'train.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214b96b6",
   "metadata": {},
   "source": [
    "# Pre process the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0b517b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''拆开看我就知道怎么处理'''\n",
    "df2 = pd.DataFrame()\n",
    "\n",
    "# 首先按照\"###\" 把文章分成一块一块的 \n",
    "nums = []\n",
    "for num, line in enumerate(lines):\n",
    "    if line.startswith(\"###\"):\n",
    "            nums.append(num)\n",
    "            \n",
    "nums.append(len(lines))\n",
    "\n",
    "\n",
    "# 然后把每一块变成df， 装入想要的features\n",
    "for n in range(len(nums) -1):\n",
    "    paper = lines[nums[n]: nums[n+1]]\n",
    "    \n",
    "    \n",
    "    df1 = pd.DataFrame()\n",
    "\n",
    "    rsearch_num = []\n",
    "    subject = []\n",
    "    main = []\n",
    "\n",
    "\n",
    "    rsearch_num.append(paper[0])\n",
    "    for l in paper[1:-1]:\n",
    "        l2 = l.split('\\t')\n",
    "        subject.append(l2[0])\n",
    "        main.append(l2[1])\n",
    "\n",
    "    df1['research num'] =  rsearch_num* len(main)\n",
    "    df1['subject'] =  subject\n",
    "    df1['main'] =  main\n",
    "    df1['line_number'] =  list(range(len(paper[1:-1])))\n",
    "    df1['total_lines'] =  len(paper[1:-1])\n",
    "    \n",
    "    # 然后再把这些小的df合到一起\n",
    "    df2 = pd.concat([df2,df1])\n",
    "    \n",
    "df2 = df2.reset_index()\n",
    "df2 = df2[['research num', 'subject', 'main', 'line_number','total_lines']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9889237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''改成function 因为有多个 text file要处理'''\n",
    "\n",
    "def Process_textfile(file):\n",
    "    \n",
    "    with open(file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "        df2 = pd.DataFrame()\n",
    "\n",
    "    # 首先按照\"###\" 把文章分成一块一块的 \n",
    "    nums = []\n",
    "    for num, line in enumerate(lines):\n",
    "        if line.startswith(\"###\"):\n",
    "                nums.append(num)\n",
    "\n",
    "    nums.append(len(lines))\n",
    "\n",
    "\n",
    "    # 然后把每一块变成df， 装入想要的features\n",
    "    for n in range(len(nums) -1):\n",
    "        paper = lines[nums[n]: nums[n+1]]\n",
    "\n",
    "\n",
    "        df1 = pd.DataFrame()\n",
    "\n",
    "        rsearch_num = []\n",
    "        subject = []\n",
    "        main = []\n",
    "\n",
    "\n",
    "        rsearch_num.append(paper[0])\n",
    "        for l in paper[1:-1]:\n",
    "            l2 = l.split('\\t')\n",
    "            subject.append(l2[0])\n",
    "            main.append(l2[1])\n",
    "\n",
    "        df1['research num'] =  rsearch_num* len(main)\n",
    "        df1['subject'] =  subject\n",
    "        df1['text'] =  main\n",
    "        df1['line_number'] =  list(range(len(paper[1:-1])))\n",
    "        df1['total_lines'] =  len(paper[1:-1])\n",
    "\n",
    "        # 然后再把这些小的df合到一起\n",
    "        df2 = pd.concat([df2,df1])\n",
    "\n",
    "    df2 = df2.reset_index()\n",
    "    df2 = df2[['research num', 'subject', 'text', 'line_number','total_lines']]\n",
    "\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f922d5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180040, 30212, 30135)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = Process_textfile('test.txt')\n",
    "val_df = Process_textfile('dev.txt')\n",
    "train_df = Process_textfile('train.txt')\n",
    "\n",
    "len(train_df), len(val_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00a4f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = Process_textfile('train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb327cb",
   "metadata": {},
   "source": [
    "# Data EDA (Exploratory data analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6120f907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "952a2db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>research num</th>\n",
       "      <th>subject</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>###24293578\\n</td>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>To investigate the efficacy of @ weeks of dail...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>###24293578\\n</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>A total of @ patients with primary knee OA wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>###24293578\\n</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>Outcome measures included pain reduction and i...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>###24293578\\n</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>Pain was assessed using the visual analog pain...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>###24293578\\n</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>Secondary outcome measures included the Wester...</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180035</th>\n",
       "      <td>###26227186\\n</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>For the absolute change in percent atheroma vo...</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180036</th>\n",
       "      <td>###26227186\\n</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>For PAV , a significantly greater percentage o...</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180037</th>\n",
       "      <td>###26227186\\n</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>Both strategies had acceptable side effect pro...</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180038</th>\n",
       "      <td>###26227186\\n</td>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>Compared with standard statin monotherapy , th...</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180039</th>\n",
       "      <td>###26227186\\n</td>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>( Plaque Regression With Cholesterol Absorptio...</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180040 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         research num      subject  \\\n",
       "0       ###24293578\\n    OBJECTIVE   \n",
       "1       ###24293578\\n      METHODS   \n",
       "2       ###24293578\\n      METHODS   \n",
       "3       ###24293578\\n      METHODS   \n",
       "4       ###24293578\\n      METHODS   \n",
       "...               ...          ...   \n",
       "180035  ###26227186\\n      RESULTS   \n",
       "180036  ###26227186\\n      RESULTS   \n",
       "180037  ###26227186\\n      RESULTS   \n",
       "180038  ###26227186\\n  CONCLUSIONS   \n",
       "180039  ###26227186\\n  CONCLUSIONS   \n",
       "\n",
       "                                                     text  line_number  \\\n",
       "0       To investigate the efficacy of @ weeks of dail...            0   \n",
       "1       A total of @ patients with primary knee OA wer...            1   \n",
       "2       Outcome measures included pain reduction and i...            2   \n",
       "3       Pain was assessed using the visual analog pain...            3   \n",
       "4       Secondary outcome measures included the Wester...            4   \n",
       "...                                                   ...          ...   \n",
       "180035  For the absolute change in percent atheroma vo...            7   \n",
       "180036  For PAV , a significantly greater percentage o...            8   \n",
       "180037  Both strategies had acceptable side effect pro...            9   \n",
       "180038  Compared with standard statin monotherapy , th...           10   \n",
       "180039  ( Plaque Regression With Cholesterol Absorptio...           11   \n",
       "\n",
       "        total_lines  \n",
       "0                12  \n",
       "1                12  \n",
       "2                12  \n",
       "3                12  \n",
       "4                12  \n",
       "...             ...  \n",
       "180035           12  \n",
       "180036           12  \n",
       "180037           12  \n",
       "180038           12  \n",
       "180039           12  \n",
       "\n",
       "[180040 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09aaaf6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "METHODS        59353\n",
       "RESULTS        57953\n",
       "CONCLUSIONS    27168\n",
       "BACKGROUND     21727\n",
       "OBJECTIVE      13839\n",
       "Name: subject, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# objects 稍微少一点 \n",
    "train_df.subject.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bf947bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 看来 10-12个subject的文章最多 \n",
    "train_df.total_lines.plot.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9986016f",
   "metadata": {},
   "source": [
    "# Make numeric labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c944ef2a",
   "metadata": {},
   "source": [
    "## Dummies variable (one hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7b39fc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = set(train_df.subject.tolist())\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aa0eff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BACKGROUND</th>\n",
       "      <th>CONCLUSIONS</th>\n",
       "      <th>METHODS</th>\n",
       "      <th>OBJECTIVE</th>\n",
       "      <th>RESULTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180035</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180036</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180037</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180038</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180039</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180040 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        BACKGROUND  CONCLUSIONS  METHODS  OBJECTIVE  RESULTS\n",
       "0                0            0        0          1        0\n",
       "1                0            0        1          0        0\n",
       "2                0            0        1          0        0\n",
       "3                0            0        1          0        0\n",
       "4                0            0        1          0        0\n",
       "...            ...          ...      ...        ...      ...\n",
       "180035           0            0        0          0        1\n",
       "180036           0            0        0          0        1\n",
       "180037           0            0        0          0        1\n",
       "180038           0            1        0          0        0\n",
       "180039           0            1        0          0        0\n",
       "\n",
       "[180040 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(train_df.subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ce3baf",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b91b787",
   "metadata": {},
   "source": [
    "## Label encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f044fb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, ..., 4, 1, 1])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_df[\"subject\"].to_numpy())\n",
    "val_labels_encoded = label_encoder.transform(val_df[\"subject\"].to_numpy())\n",
    "test_labels_encoded = label_encoder.transform(test_df[\"subject\"].to_numpy())\n",
    "\n",
    "train_labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5e3bc5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa443e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "METHODS        59353\n",
       "RESULTS        57953\n",
       "CONCLUSIONS    27168\n",
       "BACKGROUND     21727\n",
       "OBJECTIVE      13839\n",
       "Name: subject, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.subject.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9646f7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 21727), (1, 27168), (2, 59353), (3, 13839), (4, 57953)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "sorted(Counter(train_labels_encoded).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c47eaffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 3621), (1, 4571), (2, 9897), (3, 2333), (4, 9713)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(Counter(test_labels_encoded).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6385f5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0dc721",
   "metadata": {},
   "source": [
    "# Model 0: Getting a baseline （TfidfVectorizer &MultinomialNB） "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e8b2955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline\n",
    "model_0 = Pipeline([\n",
    "  (\"tf-idf\", TfidfVectorizer()),\n",
    "  (\"classfier\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(X=train_df.text, \n",
    "            y=train_labels_encoded);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86deb778",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.716674962667994"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate baseline on validation dataset\n",
    "model_0.score(X=test_df.text,y=test_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a4e7128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.716674962667994"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "ypred = model_0.predict(test_df.text)\n",
    "accuracy_score(test_labels_encoded,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3369455a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BACKGROUND</th>\n",
       "      <td>0.647239</td>\n",
       "      <td>0.466170</td>\n",
       "      <td>0.541981</td>\n",
       "      <td>3621.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONCLUSIONS</th>\n",
       "      <td>0.631453</td>\n",
       "      <td>0.577992</td>\n",
       "      <td>0.603541</td>\n",
       "      <td>4571.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>METHODS</th>\n",
       "      <td>0.722620</td>\n",
       "      <td>0.877337</td>\n",
       "      <td>0.792498</td>\n",
       "      <td>9897.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OBJECTIVE</th>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.126018</td>\n",
       "      <td>0.215227</td>\n",
       "      <td>2333.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RESULTS</th>\n",
       "      <td>0.758602</td>\n",
       "      <td>0.853495</td>\n",
       "      <td>0.803256</td>\n",
       "      <td>9713.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             precision    recall  f1-score  support\n",
       "BACKGROUND    0.647239  0.466170  0.541981   3621.0\n",
       "CONCLUSIONS   0.631453  0.577992  0.603541   4571.0\n",
       "METHODS       0.722620  0.877337  0.792498   9897.0\n",
       "OBJECTIVE     0.736842  0.126018  0.215227   2333.0\n",
       "RESULTS       0.758602  0.853495  0.803256   9713.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "c = classification_report(test_labels_encoded, ypred, digits = 4,output_dict=True)\n",
    "c = pd.DataFrame(c).transpose()\n",
    "c = c.iloc[:-3,:]\n",
    "c.index = label_encoder.classes_\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d7f53",
   "metadata": {},
   "source": [
    "## Baseline accuary -> 0.72"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc109a",
   "metadata": {},
   "source": [
    "# Preparing data from deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6b930d",
   "metadata": {},
   "source": [
    "## Prepare the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a54930e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sentence_lens'] = train_df['text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5f68f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>research num</th>\n",
       "      <th>subject</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "      <th>sentence_lens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>###24293578\\n</td>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>To investigate the efficacy of @ weeks of dail...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>###24293578\\n</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>A total of @ patients with primary knee OA wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>###24293578\\n</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>Outcome measures included pain reduction and i...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>###24293578\\n</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>Pain was assessed using the visual analog pain...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>###24293578\\n</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>Secondary outcome measures included the Wester...</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180035</th>\n",
       "      <td>###26227186\\n</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>For the absolute change in percent atheroma vo...</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180036</th>\n",
       "      <td>###26227186\\n</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>For PAV , a significantly greater percentage o...</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180037</th>\n",
       "      <td>###26227186\\n</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>Both strategies had acceptable side effect pro...</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180038</th>\n",
       "      <td>###26227186\\n</td>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>Compared with standard statin monotherapy , th...</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180039</th>\n",
       "      <td>###26227186\\n</td>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>( Plaque Regression With Cholesterol Absorptio...</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180040 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         research num      subject  \\\n",
       "0       ###24293578\\n    OBJECTIVE   \n",
       "1       ###24293578\\n      METHODS   \n",
       "2       ###24293578\\n      METHODS   \n",
       "3       ###24293578\\n      METHODS   \n",
       "4       ###24293578\\n      METHODS   \n",
       "...               ...          ...   \n",
       "180035  ###26227186\\n      RESULTS   \n",
       "180036  ###26227186\\n      RESULTS   \n",
       "180037  ###26227186\\n      RESULTS   \n",
       "180038  ###26227186\\n  CONCLUSIONS   \n",
       "180039  ###26227186\\n  CONCLUSIONS   \n",
       "\n",
       "                                                     text  line_number  \\\n",
       "0       To investigate the efficacy of @ weeks of dail...            0   \n",
       "1       A total of @ patients with primary knee OA wer...            1   \n",
       "2       Outcome measures included pain reduction and i...            2   \n",
       "3       Pain was assessed using the visual analog pain...            3   \n",
       "4       Secondary outcome measures included the Wester...            4   \n",
       "...                                                   ...          ...   \n",
       "180035  For the absolute change in percent atheroma vo...            7   \n",
       "180036  For PAV , a significantly greater percentage o...            8   \n",
       "180037  Both strategies had acceptable side effect pro...            9   \n",
       "180038  Compared with standard statin monotherapy , th...           10   \n",
       "180039  ( Plaque Regression With Cholesterol Absorptio...           11   \n",
       "\n",
       "        total_lines  sentence_lens  \n",
       "0                12             49  \n",
       "1                12             27  \n",
       "2                12             15  \n",
       "3                12             14  \n",
       "4                12             35  \n",
       "...             ...            ...  \n",
       "180035           12             95  \n",
       "180036           12             28  \n",
       "180037           12             18  \n",
       "180038           12             30  \n",
       "180039           12             21  \n",
       "\n",
       "[180040 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "65bacadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.338269273494777"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#平均 26个 单词一句\n",
    "np.average(train_df['sentence_lens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f2b1842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 大多数都在0-50句\n",
    "plt.hist(train_df['sentence_lens'], bins=7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c81673a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#covers 95% of the lengths\n",
    "output_seq_len = int(np.percentile(train_df['sentence_lens'], 95))\n",
    "output_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8db5b940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.25"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.5*2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d3a170",
   "metadata": {},
   "source": [
    "## Create text level (word) vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb189fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "Attributable costs and length of stay for each complication were calculated by multiplying the independent cost of each event by its frequency in the treatment group .\n",
      "\n",
      "\n",
      "Length of text: 27\n",
      "\n",
      "Vectorized text:\n",
      "[[ 3289   577     3   523     4   603    11   122  1101     9   757    22\n",
      "  29957     2   556   608     4   122   675    22   308   400     5     2\n",
      "     19    13     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# 总共的词汇量， vocabulary size of the PubMed 20k dataset as 68,000\n",
    "max_tokens = 68000\n",
    "train_sentences = train_df['text']\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_tokens, \n",
    "                                    output_sequence_length=55)\n",
    "\n",
    "# Adapt text vectorizer to training sentences\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "# Test out text vectorizer\n",
    "import random\n",
    "target_sentence = random.choice(train_sentences)\n",
    "print(f\"Text:\\n{target_sentence}\")\n",
    "print(f\"\\nLength of text: {len(target_sentence.split())}\")\n",
    "print(f\"\\nVectorized text:\\n{text_vectorizer([target_sentence])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c85ffdf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[UNK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64836</th>\n",
       "      <td>aainduced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64837</th>\n",
       "      <td>aaigroup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64838</th>\n",
       "      <td>aachener</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64839</th>\n",
       "      <td>aachen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64840</th>\n",
       "      <td>aaacp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64841 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "0               \n",
       "1          [UNK]\n",
       "2            the\n",
       "3            and\n",
       "4             of\n",
       "...          ...\n",
       "64836  aainduced\n",
       "64837   aaigroup\n",
       "64838   aachener\n",
       "64839     aachen\n",
       "64840      aaacp\n",
       "\n",
       "[64841 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(text_vectorizer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdfee2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b6e6932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([b'the', b'and', b'of', ..., b'aachener', b'aachen', b'aaacp'],\n",
       "       dtype=object)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de357e3c",
   "metadata": {},
   "source": [
    "## Find the most common words in our word level vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe460f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 64841\n",
      "Most common words in the vocabulary: ['', '[UNK]', 'the', 'and', 'of']\n",
      "Least common words in the vocabulary: ['aainduced', 'aaigroup', 'aachener', 'aachen', 'aaacp']\n"
     ]
    }
   ],
   "source": [
    "rct_20k_text_vocab = text_vectorizer.get_vocabulary()\n",
    "print(f\"Number of words in vocabulary: {len(rct_20k_text_vocab)}\"), \n",
    "print(f\"Most common words in the vocabulary: {rct_20k_text_vocab[:5]}\")\n",
    "print(f\"Least common words in the vocabulary: {rct_20k_text_vocab[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77a2813f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'text_vectorization',\n",
       " 'trainable': True,\n",
       " 'batch_input_shape': (None, None),\n",
       " 'dtype': 'string',\n",
       " 'max_tokens': 68000,\n",
       " 'standardize': 'lower_and_strip_punctuation',\n",
       " 'split': 'whitespace',\n",
       " 'ngrams': None,\n",
       " 'output_mode': 'int',\n",
       " 'output_sequence_length': 55,\n",
       " 'pad_to_max_tokens': False,\n",
       " 'sparse': False,\n",
       " 'ragged': False,\n",
       " 'vocabulary': None,\n",
       " 'idf_weights': None}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the config of our text vectorizer\n",
    "text_vectorizer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b565d41",
   "metadata": {},
   "source": [
    "## Create an Embedding layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd721f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence before vectorization:\n",
      "The top two methods for recruitment were mass mailing followed by email ; together they were cited by @ % of those recruited .\n",
      "\n",
      "\n",
      "Sentence after vectorization (before embedding):\n",
      "[[    2  3833    51   578    11   940     9   385 22611   284    22  4281\n",
      "   2004   316     9 10417    22     4   125   404     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0]]\n",
      "\n",
      "vectorization sentence shape: (1, 55)\n",
      "\n",
      "------------\n",
      "\n",
      "Sentence after embedding:\n",
      "[[[-0.01555037  0.02877511 -0.00347774 ... -0.03784745  0.03308256\n",
      "   -0.03672491]\n",
      "  [-0.03711827  0.01279615 -0.01812196 ...  0.01001102  0.01140619\n",
      "    0.01739408]\n",
      "  [-0.03450936  0.00593271 -0.01724952 ...  0.04704529  0.01969621\n",
      "   -0.01385029]\n",
      "  ...\n",
      "  [-0.03125256  0.00108073 -0.02026033 ...  0.0032765  -0.00070886\n",
      "   -0.03109043]\n",
      "  [-0.03125256  0.00108073 -0.02026033 ...  0.0032765  -0.00070886\n",
      "   -0.03109043]\n",
      "  [-0.03125256  0.00108073 -0.02026033 ...  0.0032765  -0.00070886\n",
      "   -0.03109043]]]\n",
      "\n",
      "Embedded sentence shape: (1, 55, 128)\n"
     ]
    }
   ],
   "source": [
    "# Create token embedding layer 相当于是把每个单词每个都转成128维度，比如这句话10个单词就是 10*128\n",
    "token_embed = tf.keras.layers.Embedding(input_dim=len(rct_20k_text_vocab), # length of vocabulary\n",
    "                               output_dim=128, \n",
    "                               # 意思是告诉 network 这些是padding，不是真正的数字， 所以不需要去计算这些 (update weights)\n",
    "                               mask_zero=True,\n",
    "                               name=\"token_embedding\") \n",
    "\n",
    "train_sentences = train_df['text']\n",
    "target_sentence = random.choice(train_sentences)\n",
    "\n",
    "\n",
    "# Show example embedding\n",
    "print(f\"Sentence before vectorization:\\n{target_sentence}\\n\")\n",
    "vectorized_sentence = text_vectorizer([target_sentence])\n",
    "print(f\"Sentence after vectorization (before embedding):\\n{vectorized_sentence}\\n\")\n",
    "print(f\"vectorization sentence shape: {vectorized_sentence.shape}\")\n",
    "\n",
    "print( )\n",
    "print('------------')\n",
    "print( )\n",
    "\n",
    "embedded_sentence = token_embed(vectorized_sentence)\n",
    "print(f\"Sentence after embedding:\\n{embedded_sentence}\\n\")\n",
    "print(f\"Embedded sentence shape: {embedded_sentence.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5478beaf",
   "metadata": {},
   "source": [
    "## Create fast loading dataset with the tensorflow tf.data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09e0c2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((), (5,)), types: (tf.string, tf.uint8)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn our data into TensorFlow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_df['text'], pd.get_dummies(train_df.subject)))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((val_df['text'], pd.get_dummies(val_df.subject)))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_df['text'], pd.get_dummies(test_df.subject)))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ea64c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180040, 30212, 30135)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(val_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be3ed3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None,), (None, 5)), types: (tf.string, tf.uint8)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUTOTUNE 意思是自动来 how many sample should we prefetch at a time, 这里就是 as much as possible\n",
    "# 而且这里的数据不用shuffle， 因为 order matters here \n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a5a83c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5627, 945, 942, 180064)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(train_dataset) *32 -> 180064\n",
    "len(train_dataset),len(valid_dataset),len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7203187",
   "metadata": {},
   "source": [
    "# Model 1: Conv1D with token embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1569d73b",
   "metadata": {},
   "source": [
    "## No shuffle train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5bcf16ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 41s 72ms/step - loss: 0.5559 - accuracy: 0.8078 - val_loss: 0.5704 - val_accuracy: 0.7992\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 41s 73ms/step - loss: 0.3846 - accuracy: 0.8699 - val_loss: 0.5771 - val_accuracy: 0.7972\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 41s 73ms/step - loss: 0.3550 - accuracy: 0.8813 - val_loss: 0.5937 - val_accuracy: 0.7906\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None,)]                 0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 55)               0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " token_embedding (Embedding)  (None, 55, 128)          8299648   \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, 55, 64)            41024     \n",
      "                                                                 \n",
      " global_average_pooling1d_8   (None, 64)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,340,997\n",
      "Trainable params: 8,340,997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#train_dataset = train_dataset.batch(32).prefetch(3)\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape = (), dtype= tf.string)\n",
    "text_vectors = text_vectorizer(inputs)\n",
    "embedding = token_embed(text_vectors)\n",
    "\n",
    "x = tf.keras.layers.Conv1D(filters = 64, kernel_size = 5, padding = 'same',activation = 'relu')(embedding)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(num_classes, activation = 'softmax')(x)\n",
    "\n",
    "\n",
    "model1_no_shuffle = tf.keras.Model(inputs, outputs)\n",
    "model1_no_shuffle.compile(loss = 'categorical_crossentropy', \n",
    "                          optimizer = tf.keras.optimizers.Adam(),\n",
    "                          metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "model1_no_shuffle.fit(train_dataset,\n",
    "                      validation_data=valid_dataset,\n",
    "                      epochs= 3,\n",
    "                      batch_size = 32,\n",
    "                      \n",
    "                      \n",
    "                      # only take 10% of data, 因为数据量太大了 几十万条text, 这里可以选择多少数据从 training data里面训练\n",
    "                      steps_per_epoch=int(0.1 * len(train_dataset)),\n",
    "                      validation_steps=int(0.1 * len(valid_dataset))\n",
    "                     )\n",
    "                     \n",
    "\n",
    "\n",
    "model1_no_shuffle.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2dc11aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 3, ..., 4, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_pred_probs = model1_no_shuffle.predict(valid_dataset)\n",
    "model_1_pred_probs.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1b18716a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6104    0.5741    0.5917      3449\n",
      "           1     0.6914    0.7124    0.7017      4582\n",
      "           2     0.8667    0.8687    0.8677      9964\n",
      "           3     0.6027    0.5619    0.5816      2376\n",
      "           4     0.8492    0.8668    0.8579      9841\n",
      "\n",
      "    accuracy                         0.7866     30212\n",
      "   macro avg     0.7241    0.7168    0.7201     30212\n",
      "weighted avg     0.7844    0.7866    0.7853     30212\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_labels_encoded, model_1_pred_probs.argmax(axis = 1), output_dict = False, digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2deadda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BACKGROUND</th>\n",
       "      <td>0.610358</td>\n",
       "      <td>0.574079</td>\n",
       "      <td>0.591663</td>\n",
       "      <td>3449.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONCLUSIONS</th>\n",
       "      <td>0.691379</td>\n",
       "      <td>0.712353</td>\n",
       "      <td>0.701709</td>\n",
       "      <td>4582.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>METHODS</th>\n",
       "      <td>0.866727</td>\n",
       "      <td>0.868727</td>\n",
       "      <td>0.867726</td>\n",
       "      <td>9964.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OBJECTIVE</th>\n",
       "      <td>0.602709</td>\n",
       "      <td>0.561869</td>\n",
       "      <td>0.581573</td>\n",
       "      <td>2376.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RESULTS</th>\n",
       "      <td>0.849179</td>\n",
       "      <td>0.866782</td>\n",
       "      <td>0.857890</td>\n",
       "      <td>9841.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             precision    recall  f1-score  support\n",
       "BACKGROUND    0.610358  0.574079  0.591663   3449.0\n",
       "CONCLUSIONS   0.691379  0.712353  0.701709   4582.0\n",
       "METHODS       0.866727  0.868727  0.867726   9964.0\n",
       "OBJECTIVE     0.602709  0.561869  0.581573   2376.0\n",
       "RESULTS       0.849179  0.866782  0.857890   9841.0"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('classification report:')\n",
    "c = classification_report(val_labels_encoded, model_1_pred_probs.argmax(axis = 1), digits = 4,output_dict=True)\n",
    "c = pd.DataFrame(c).transpose()\n",
    "c = c.iloc[:-3,:]\n",
    "c.index = label_encoder.classes_\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ac60a1bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BACKGROUND'], dtype=object)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.inverse_transform([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5450eac",
   "metadata": {},
   "source": [
    "### Model 1 no shuffle accuary -> 0.786"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b2040e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de35ff9",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a306c2",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc89bfe",
   "metadata": {},
   "source": [
    "## Shuffle train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3cb6c5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 31s 55ms/step - loss: 0.6627 - accuracy: 0.7614 - val_loss: 0.5857 - val_accuracy: 0.7896\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 41s 74ms/step - loss: 0.5507 - accuracy: 0.8018 - val_loss: 0.5731 - val_accuracy: 0.7919\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 38s 68ms/step - loss: 0.5436 - accuracy: 0.8106 - val_loss: 0.5704 - val_accuracy: 0.7965\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_21 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_2 (TextV  (None, 55)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " token_embedding (Embedding)  (None, 55, 128)          8299648   \n",
      "                                                                 \n",
      " conv1d_13 (Conv1D)          (None, 55, 64)            41024     \n",
      "                                                                 \n",
      " global_average_pooling1d_9   (None, 64)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,340,997\n",
      "Trainable params: 8,340,997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# shuffle the traning dataset\n",
    "train_df = train_df.sample(len(train_df))\n",
    "\n",
    "\n",
    "# Turn our data into TensorFlow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_df['text'], pd.get_dummies(train_df.subject)))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((val_df['text'], pd.get_dummies(val_df.subject)))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_df['text'], pd.get_dummies(test_df.subject)))\n",
    "\n",
    "# put data into batch \n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# model\n",
    "inputs = tf.keras.layers.Input(shape = (1,), dtype= tf.string)\n",
    "text_vectors = text_vectorizer(inputs)\n",
    "embedding = token_embed(text_vectors)\n",
    "\n",
    "x = tf.keras.layers.Conv1D(filters = 64, kernel_size = 5, padding = 'same',activation = 'relu')(embedding)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(len(num_classes), activation = 'softmax')(x)\n",
    "\n",
    "\n",
    "model1_shuffle = tf.keras.Model(inputs, outputs)\n",
    "model1_shuffle.compile(loss = 'categorical_crossentropy', \n",
    "                          optimizer = tf.keras.optimizers.Adam(),\n",
    "                          metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "model1_shuffle.fit(train_dataset,\n",
    "                   validation_data=valid_dataset,\n",
    "                   epochs= 3,\n",
    "\n",
    "                   # only take 10% of data, 因为数据量太大了 几十万条text \n",
    "                   steps_per_epoch=int(0.1 * len(train_dataset)),\n",
    "                   validation_steps=int(0.1 * len(valid_dataset))\n",
    "                   )\n",
    "                     \n",
    "\n",
    "\n",
    "model1_shuffle.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac40575c",
   "metadata": {},
   "source": [
    "### Model 1 shuffle accuary -> 0.7965"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a655e",
   "metadata": {},
   "source": [
    "# Model 2: transfer learning model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7c43015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained TensorFlow Hub USE\n",
    "import tensorflow_hub as hub\n",
    "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        trainable=False,\n",
    "                                        name=\"universal_sentence_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5077d221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random training sentence:\n",
      "A comparison of the screening rates will then take place , using the faecal occult blood test of the patients from the control and the intervention groups .\n",
      "\n",
      "\n",
      "Sentence after embedding:\n",
      "[-0.04340596 -0.00410624  0.04453267 -0.02551643  0.00866798 -0.04148094\n",
      "  0.07219423 -0.03838151 -0.05621168 -0.00078156  0.09520157  0.04321394\n",
      "  0.04750543  0.03999333 -0.03068188 -0.01753978 -0.09501836 -0.06980894\n",
      " -0.07800989  0.05257524 -0.03943029  0.02266692 -0.08308131 -0.01921579\n",
      "  0.05804289 -0.05223611 -0.06536747  0.07417352  0.04150463  0.01932045] (truncated output)...\n",
      "\n",
      "Length of sentence embedding:\n",
      "(512,)\n"
     ]
    }
   ],
   "source": [
    "# Test out the embedding on a random sentence\n",
    "random_training_sentence = random.choice(train_df['text'])\n",
    "print(f\"Random training sentence:\\n{random_training_sentence}\\n\")\n",
    "use_embedded_sentence = tf_hub_embedding_layer([random_training_sentence])\n",
    "print(f\"Sentence after embedding:\\n{use_embedded_sentence[0][:30]} (truncated output)...\\n\")\n",
    "print(f\"Length of sentence embedding:\\n{use_embedded_sentence[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85a8efbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5627\n",
      "Epoch 1/3\n",
      "562/562 [==============================] - 6s 8ms/step - loss: 0.9210 - accuracy: 0.6481 - val_loss: 0.8010 - val_accuracy: 0.6835\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 4s 8ms/step - loss: 0.7748 - accuracy: 0.6998 - val_loss: 0.7577 - val_accuracy: 0.7041\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 5s 8ms/step - loss: 0.7565 - accuracy: 0.7117 - val_loss: 0.7423 - val_accuracy: 0.7144\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None,)]                 0         \n",
      "                                                                 \n",
      " universal_sentence_encoder   (None, 512)              256797824 \n",
      " (KerasLayer)                                                    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,864,133\n",
      "Trainable params: 66,309\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# functional API + trainable=False\n",
    "\n",
    "print(len(train_dataset))\n",
    "\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "pretrained_embedding = tf_hub_embedding_layer(inputs)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(pretrained_embedding)\n",
    "outputs = tf.keras.layers.Dense(5, activation=\"softmax\")(x) # create the output layer\n",
    "\n",
    "model_2 = tf.keras.Model(inputs=inputs,\n",
    "                        outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model_2.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "model_2.fit(train_dataset,\n",
    "            validation_data=valid_dataset,\n",
    "            epochs = 3,\n",
    "            steps_per_epoch=int(0.1 * len(train_dataset)),\n",
    "            validation_steps=int(0.1 * len(valid_dataset)),\n",
    "           )\n",
    "\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bcf2dde6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5627\n",
      "Epoch 1/3\n",
      "227/562 [===========>..................] - ETA: 6:04 - loss: 0.9370 - accuracy: 0.6322"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-d882164e4685>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m model22.fit(train_dataset,\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3128\u001b[0m       (graph_function,\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1959\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# sequential API\n",
    "# trainable=True 太慢 所以关掉了\n",
    "\n",
    "\n",
    "\n",
    "print(len(train_dataset))\n",
    "\n",
    "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        trainable=True,\n",
    "                                        input_shape=[], \n",
    "                                        dtype=tf.string,\n",
    "                                        name=\"universal_sentence_encoder\")\n",
    "\n",
    "model22 = tf.keras.Sequential([\n",
    "    tf_hub_embedding_layer,\n",
    "\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(5, activation=\"softmax\")\n",
    "])\n",
    "model22.compile(loss ='categorical_crossentropy', \n",
    "               optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "               metrics =['accuracy'])\n",
    "\n",
    "\n",
    "model22.fit(train_dataset,\n",
    "            validation_data=valid_dataset,\n",
    "            epochs = 3,\n",
    "            steps_per_epoch=int(0.1 * len(train_dataset)),\n",
    "            validation_steps=int(0.1 * len(valid_dataset)),\n",
    "           batch_size = 32)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "print(model22.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627e4cce",
   "metadata": {},
   "source": [
    "## Model2 transfer learning -> 0.71"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256661b6",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc340f03",
   "metadata": {},
   "source": [
    "# Model 3: Conv1D with character embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d65875f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T h e s e   p a t i e n t s   w e r e   c o m p a r e d   w i t h   @   c o n v e n t i o n a l l y   t r e a t e d   p a t i e n t s   w i t h   P C   . \\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_chars(text):\n",
    "      return \" \".join(list(text))\n",
    "\n",
    "    \n",
    "random_training_sentence =  train_df['text'].sample(1).tolist()[0]\n",
    "# Test splitting non-character-level sequence into characters\n",
    "split_chars(random_training_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d45e420",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " 'h',\n",
       " 'i',\n",
       " 'r',\n",
       " 'd',\n",
       " ' ',\n",
       " ',',\n",
       " ' ',\n",
       " 'w',\n",
       " 'e',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " 'c',\n",
       " 'l',\n",
       " 'u',\n",
       " 'd',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'c',\n",
       " 'o',\n",
       " 'm',\n",
       " 'p',\n",
       " 'r',\n",
       " 'e',\n",
       " 'h',\n",
       " 'e',\n",
       " 'n',\n",
       " 's',\n",
       " 'i',\n",
       " 'v',\n",
       " 'e',\n",
       " ' ',\n",
       " 'p',\n",
       " 'r',\n",
       " 'o',\n",
       " 'c',\n",
       " 'e',\n",
       " 's',\n",
       " 's',\n",
       " ' ',\n",
       " 'e',\n",
       " 'v',\n",
       " 'a',\n",
       " 'l',\n",
       " 'u',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'r',\n",
       " 'a',\n",
       " 'i',\n",
       " 's',\n",
       " 'e',\n",
       " ' ',\n",
       " 'e',\n",
       " 'x',\n",
       " 't',\n",
       " 'e',\n",
       " 'r',\n",
       " 'n',\n",
       " 'a',\n",
       " 'l',\n",
       " ' ',\n",
       " 'v',\n",
       " 'a',\n",
       " 'l',\n",
       " 'i',\n",
       " 'd',\n",
       " 'i',\n",
       " 't',\n",
       " 'y',\n",
       " ' ',\n",
       " '.',\n",
       " '\\n']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(random_training_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c6f7f6",
   "metadata": {},
   "source": [
    "## Crate train, test , validation character "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "718c95a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180040"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split sequence-level data splits into character-level data splits\n",
    "train_chars = [split_chars(sentence) for sentence in train_df['text']]\n",
    "val_chars = [split_chars(sentence) for sentence in val_df['text']]\n",
    "test_chars = [split_chars(sentence) for sentence in test_df['text']]\n",
    "\n",
    "len(train_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "767cd632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T o   i n v e s t i g a t e   t h e   e f f i c a c y   o f   @   w e e k s   o f   d a i l y   l o w - d o s e   o r a l   p r e d n i s o l o n e   i n   i m p r o v i n g   p a i n   ,   m o b i l i t y   ,   a n d   s y s t e m i c   l o w - g r a d e   i n f l a m m a t i o n   i n   t h e   s h o r t   t e r m   a n d   w h e t h e r   t h e   e f f e c t   w o u l d   b e   s u s t a i n e d   a t   @   w e e k s   i n   o l d e r   a d u l t s   w i t h   m o d e r a t e   t o   s e v e r e   k n e e   o s t e o a r t h r i t i s   (   O A   )   . \\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chars[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "281d8cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example - the len of a string， 空格这些都算所以这个列子就算9个caracters \n",
    "len('haha haha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e79b1458",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150.3662574983337"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 求的是平均句子的 caracters 个数\n",
    "anp.mean([len(i) for i in train_df['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "11b7cbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.41175e+05, 3.71110e+04, 1.60000e+03, 1.27000e+02, 2.10000e+01,\n",
       "        5.00000e+00, 1.00000e+00]),\n",
       " array([   2.        ,  199.85714286,  397.71428571,  595.57142857,\n",
       "         793.42857143,  991.28571429, 1189.14285714, 1387.        ]),\n",
       " <BarContainer object of 7 artists>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD4CAYAAADy46FuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAW1UlEQVR4nO3df6zd9X3f8edrdkMgGcSAodS2dp1idQO0LcEipJ2qaO7ATSLMHyA5aoa3erKG2JZ2q1I8pLIlsgRrVVrUwYQCxdAMsNx0WIlYYkGraBIxufnJr1BuCoUbHHw7U8paQWL63h/nc5Xjm+OPfe/1/UF5PqSj8z3v7/fzve/v1fV93e/38z3HqSokSTqWv7fUDUiSljeDQpLUZVBIkroMCklSl0EhSepaudQNnGxnn312jY2NLXUbkvSW8rWvfe0vqmr1qHV/54JibGyM8fHxpW5Dkt5Skvz5sdZ56UmS1GVQSJK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUdNyiS3JXkUJInRqz7tSSV5Oyh2s4kE0meSXL5UP3iJI+3dbcmSaufkuSBVj+QZGxozLYkz7bHtnkfrSRp1k7kjOJuYPPMYpJ1wL8AXhiqXQBsBS5sY25LsqKtvh3YAWxoj+l9bgdeqarzgVuAm9u+zgRuBD4AXALcmGTV7A5PkjRfx31ndlV9efiv/CG3AJ8EHhyqbQHur6o3gOeSTACXJHkeOL2qHgVIcg9wJfBQG/Nf2vi9wO+1s43Lgf1VdbiN2c8gXO6b3SHOztj1X1jI3Z9Uz9/0kaVuQdLbwJzmKJJcAXyvqr41Y9Ua4MWh15OttqYtz6wfNaaqjgCvAmd19jWqnx1JxpOMT01NzeWQJEnHMOugSHIacAPwG6NWj6hVpz7XMUcXq+6oqo1VtXH16pGfaSVJmqO5nFH8NLAe+Fa7pLQW+HqSn2TwV/+6oW3XAi+1+toRdYbHJFkJnAEc7uxLkrSIZh0UVfV4VZ1TVWNVNcbgF/r7q+r7wD5ga7uTaT2DSevHquog8FqSS9v8wzX8aG5jHzB9R9NVwCNVVcAXgcuSrGqT2Je1miRpER13MjvJfcCHgLOTTAI3VtWdo7atqieT7AGeAo4A11XVm231tQzuoDqVwST2Q61+J3Bvm/g+zOCuKarqcJJPA19t231qemJbkrR4TuSup48dZ/3YjNe7gF0jthsHLhpRfx24+hj7vgu463g9SpIWju/MliR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSugwKSVLXcYMiyV1JDiV5Yqj2m0m+k+TbSf4oyXuG1u1MMpHkmSSXD9UvTvJ4W3drkrT6KUkeaPUDScaGxmxL8mx7bDtZBy1JOnEnckZxN7B5Rm0/cFFV/WPgT4GdAEkuALYCF7YxtyVZ0cbcDuwANrTH9D63A69U1fnALcDNbV9nAjcCHwAuAW5Msmr2hyhJmo/jBkVVfRk4PKP2pao60l5+BVjblrcA91fVG1X1HDABXJLkPOD0qnq0qgq4B7hyaMzutrwX2NTONi4H9lfV4ap6hUE4zQwsSdICOxlzFL8MPNSW1wAvDq2bbLU1bXlm/agxLXxeBc7q7OvHJNmRZDzJ+NTU1LwORpJ0tHkFRZIbgCPAZ6dLIzarTn2uY44uVt1RVRurauPq1av7TUuSZmXOQdEmlz8K/FK7nASDv/rXDW22Fnip1deOqB81JslK4AwGl7qOtS9J0iKaU1Ak2Qz8OnBFVf3N0Kp9wNZ2J9N6BpPWj1XVQeC1JJe2+YdrgAeHxkzf0XQV8EgLni8ClyVZ1SaxL2s1SdIiWnm8DZLcB3wIODvJJIM7kXYCpwD7212uX6mqf1tVTybZAzzF4JLUdVX1ZtvVtQzuoDqVwZzG9LzGncC9SSYYnElsBaiqw0k+DXy1bfepqjpqUl2StPCOGxRV9bER5Ts72+8Cdo2ojwMXjai/Dlx9jH3dBdx1vB4lSQvHd2ZLkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldxw2KJHclOZTkiaHamUn2J3m2Pa8aWrczyUSSZ5JcPlS/OMnjbd2tSdLqpyR5oNUPJBkbGrOtfY1nk2w7aUctSTphJ3JGcTeweUbteuDhqtoAPNxek+QCYCtwYRtzW5IVbcztwA5gQ3tM73M78EpVnQ/cAtzc9nUmcCPwAeAS4MbhQJIkLY7jBkVVfRk4PKO8BdjdlncDVw7V76+qN6rqOWACuCTJecDpVfVoVRVwz4wx0/vaC2xqZxuXA/ur6nBVvQLs58cDS5K0wOY6R3FuVR0EaM/ntPoa4MWh7SZbbU1bnlk/akxVHQFeBc7q7OvHJNmRZDzJ+NTU1BwPSZI0ysmezM6IWnXqcx1zdLHqjqraWFUbV69efUKNSpJOzFyD4uV2OYn2fKjVJ4F1Q9utBV5q9bUj6keNSbISOIPBpa5j7UuStIjmGhT7gOm7kLYBDw7Vt7Y7mdYzmLR+rF2eei3JpW3+4ZoZY6b3dRXwSJvH+CJwWZJVbRL7slaTJC2ilcfbIMl9wIeAs5NMMrgT6SZgT5LtwAvA1QBV9WSSPcBTwBHguqp6s+3qWgZ3UJ0KPNQeAHcC9yaZYHAmsbXt63CSTwNfbdt9qqpmTqpLkhbYcYOiqj52jFWbjrH9LmDXiPo4cNGI+uu0oBmx7i7gruP1KElaOL4zW5LUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6ppXUCT51SRPJnkiyX1J3pnkzCT7kzzbnlcNbb8zyUSSZ5JcPlS/OMnjbd2tSdLqpyR5oNUPJBmbT7+SpNmbc1AkWQP8B2BjVV0ErAC2AtcDD1fVBuDh9pokF7T1FwKbgduSrGi7ux3YAWxoj82tvh14parOB24Bbp5rv5KkuZnvpaeVwKlJVgKnAS8BW4Ddbf1u4Mq2vAW4v6reqKrngAngkiTnAadX1aNVVcA9M8ZM72svsGn6bEOStDjmHBRV9T3gt4AXgIPAq1X1JeDcqjrYtjkInNOGrAFeHNrFZKutacsz60eNqaojwKvAWTN7SbIjyXiS8ampqbkekiRphPlcelrF4C/+9cBPAe9K8vHekBG16tR7Y44uVN1RVRurauPq1av7jUuSZmU+l55+AXiuqqaq6ofA54CfBV5ul5Noz4fa9pPAuqHxaxlcqppsyzPrR41pl7fOAA7Po2dJ0izNJyheAC5NclqbN9gEPA3sA7a1bbYBD7blfcDWdifTegaT1o+1y1OvJbm07eeaGWOm93UV8Eibx5AkLZKVcx1YVQeS7AW+DhwBvgHcAbwb2JNkO4Mwubpt/2SSPcBTbfvrqurNtrtrgbuBU4GH2gPgTuDeJBMMziS2zrVfSdLczDkoAKrqRuDGGeU3GJxdjNp+F7BrRH0cuGhE/XVa0EiSlobvzJYkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklS17yCIsl7kuxN8p0kTyf5YJIzk+xP8mx7XjW0/c4kE0meSXL5UP3iJI+3dbcmSaufkuSBVj+QZGw+/UqSZm++ZxS/C/zvqvqHwD8BngauBx6uqg3Aw+01SS4AtgIXApuB25KsaPu5HdgBbGiPza2+HXilqs4HbgFunme/kqRZmnNQJDkd+HngToCq+kFV/SWwBdjdNtsNXNmWtwD3V9UbVfUcMAFckuQ84PSqerSqCrhnxpjpfe0FNk2fbUiSFsd8zijeC0wBv5/kG0k+k+RdwLlVdRCgPZ/Ttl8DvDg0frLV1rTlmfWjxlTVEeBV4KyZjSTZkWQ8yfjU1NQ8DkmSNNN8gmIl8H7g9qp6H/DXtMtMxzDqTKA69d6YowtVd1TVxqrauHr16n7XkqRZmU9QTAKTVXWgvd7LIDhebpeTaM+HhrZfNzR+LfBSq68dUT9qTJKVwBnA4Xn0LEmapTkHRVV9H3gxyc+00ibgKWAfsK3VtgEPtuV9wNZ2J9N6BpPWj7XLU68lubTNP1wzY8z0vq4CHmnzGJKkRbJynuP/PfDZJO8A/gz41wzCZ0+S7cALwNUAVfVkkj0MwuQIcF1Vvdn2cy1wN3Aq8FB7wGCi/N4kEwzOJLbOs19J0izNKyiq6pvAxhGrNh1j+13ArhH1ceCiEfXXaUEjSVoavjNbktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqmndQJFmR5BtJPt9en5lkf5Jn2/OqoW13JplI8kySy4fqFyd5vK27NUla/ZQkD7T6gSRj8+1XkjQ7J+OM4hPA00OvrwcerqoNwMPtNUkuALYCFwKbgduSrGhjbgd2ABvaY3OrbwdeqarzgVuAm09Cv5KkWZhXUCRZC3wE+MxQeQuwuy3vBq4cqt9fVW9U1XPABHBJkvOA06vq0aoq4J4ZY6b3tRfYNH22IUlaHPM9o/gd4JPA3w7Vzq2qgwDt+ZxWXwO8OLTdZKutacsz60eNqaojwKvAWTObSLIjyXiS8ampqXkekiRp2JyDIslHgUNV9bUTHTKiVp16b8zRhao7qmpjVW1cvXr1CbYjSToRK+cx9ueAK5J8GHgncHqSPwBeTnJeVR1sl5UOte0ngXVD49cCL7X62hH14TGTSVYCZwCH59GzJGmW5nxGUVU7q2ptVY0xmKR+pKo+DuwDtrXNtgEPtuV9wNZ2J9N6BpPWj7XLU68lubTNP1wzY8z0vq5qX+PHzigkSQtnPmcUx3ITsCfJduAF4GqAqnoyyR7gKeAIcF1VvdnGXAvcDZwKPNQeAHcC9yaZYHAmsXUB+n3LGrv+C0vdwqw8f9NHlroFSXNwUoKiqv4E+JO2/H+BTcfYbhewa0R9HLhoRP11WtBIkpaG78yWJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUZVBIkroMCklSl0EhSeoyKCRJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK65hwUSdYl+eMkTyd5MsknWv3MJPuTPNueVw2N2ZlkIskzSS4fql+c5PG27tYkafVTkjzQ6geSjM3jWCVJczCfM4ojwH+qqn8EXApcl+QC4Hrg4araADzcXtPWbQUuBDYDtyVZ0fZ1O7AD2NAem1t9O/BKVZ0P3ALcPI9+JUlzMOegqKqDVfX1tvwa8DSwBtgC7G6b7QaubMtbgPur6o2qeg6YAC5Jch5welU9WlUF3DNjzPS+9gKbps82JEmL46TMUbRLQu8DDgDnVtVBGIQJcE7bbA3w4tCwyVZb05Zn1o8aU1VHgFeBs0Z8/R1JxpOMT01NnYxDkiQ18w6KJO8G/hD4lar6q96mI2rVqffGHF2ouqOqNlbVxtWrVx+vZUnSLMwrKJL8BIOQ+GxVfa6VX26Xk2jPh1p9Elg3NHwt8FKrrx1RP2pMkpXAGcDh+fQsSZqd+dz1FOBO4Omq+u2hVfuAbW15G/DgUH1ru5NpPYNJ68fa5anXklza9nnNjDHT+7oKeKTNY0iSFsnKeYz9OeBfAo8n+War/WfgJmBPku3AC8DVAFX1ZJI9wFMM7pi6rqrebOOuBe4GTgUeag8YBNG9SSYYnElsnUe/kqQ5mHNQVNX/YfQcAsCmY4zZBewaUR8HLhpRf50WNJKkpeE7syVJXQaFJKnLoJAkdRkUkqQug0KS1GVQSJK6DApJUpdBIUnqMigkSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSugwKSVKXQSFJ6jIoJEldBoUkqcugkCR1GRSSpC6DQpLUtXKpGzgRSTYDvwusAD5TVTctcUuag7Hrv7DULZyw52/6yFK3IC0by/6MIskK4L8DvwhcAHwsyQVL25UkvX0s+6AALgEmqurPquoHwP3AliXuSZLeNt4Kl57WAC8OvZ4EPjC8QZIdwI728v8leWaOX+ts4C/mOHYp2O8Cyc3AW6jfxn4X1t/1fv/BsVa8FYIiI2p11IuqO4A75v2FkvGq2jjf/SwW+11Y9ruw7Hdhncx+3wqXniaBdUOv1wIvLVEvkvS281YIiq8CG5KsT/IOYCuwb4l7kqS3jWV/6amqjiT5d8AXGdwee1dVPblAX27el68Wmf0uLPtdWPa7sE5av6mq428lSXrbeitcepIkLSGDQpLUZVA0STYneSbJRJLrl0E/65L8cZKnkzyZ5BOtfmaS/Umebc+rhsbsbP0/k+TyJep7RZJvJPn8cu83yXuS7E3ynfZ9/uAy7/dX28/CE0nuS/LO5dRvkruSHEryxFBt1v0luTjJ423drUlG3SK/UP3+Zvt5+HaSP0rynuXc79C6X0tSSc5ekH6r6m3/YDBJ/l3gvcA7gG8BFyxxT+cB72/Lfx/4UwYfYfLfgOtb/Xrg5rZ8Qev7FGB9O54VS9D3fwT+J/D59nrZ9gvsBv5NW34H8J7l2i+DN54+B5zaXu8B/tVy6hf4eeD9wBNDtVn3BzwGfJDBe6geAn5xEfu9DFjZlm9e7v22+joGN/v8OXD2QvTrGcXAsvuYkKo6WFVfb8uvAU8z+GWxhcEvONrzlW15C3B/Vb1RVc8BEwyOa9EkWQt8BPjMUHlZ9pvkdAb/8O4EqKofVNVfLtd+m5XAqUlWAqcxeD/Rsum3qr4MHJ5RnlV/Sc4DTq+qR2vwW+2eoTEL3m9VfamqjrSXX2Hwvq1l229zC/BJjn4j8knt16AYGPUxIWuWqJcfk2QMeB9wADi3qg7CIEyAc9pmy+EYfofBD+zfDtWWa7/vBaaA32+Xyj6T5F3Ltd+q+h7wW8ALwEHg1ar60nLtd8hs+1vTlmfWl8IvM/iLG5Zpv0muAL5XVd+aseqk9mtQDBz3Y0KWSpJ3A38I/EpV/VVv0xG1RTuGJB8FDlXV1050yIjaYn7PVzI4jb+9qt4H/DWDSyPHstTf31UM/kpcD/wU8K4kH+8NGVFbFj/TzbH6WxZ9J7kBOAJ8dro0YrMl7TfJacANwG+MWj2iNud+DYqBZfkxIUl+gkFIfLaqPtfKL7fTR9rzoVZf6mP4OeCKJM8zuHT3z5P8Acu330lgsqoOtNd7GQTHcu33F4Dnqmqqqn4IfA742WXc77TZ9jfJjy73DNcXTZJtwEeBX2qXZ2B59vvTDP5w+Fb7d7cW+HqSn+Qk92tQDCy7jwlpdyLcCTxdVb89tGofsK0tbwMeHKpvTXJKkvXABgaTVouiqnZW1dqqGmPw/Xukqj6+jPv9PvBikp9ppU3AU8u1XwaXnC5Nclr72djEYN5qufY7bVb9tctTryW5tB3nNUNjFlwG/0narwNXVNXfDK1adv1W1eNVdU5VjbV/d5MMboD5/knvdyFm59+KD+DDDO4s+i5wwzLo558xOCX8NvDN9vgwcBbwMPBsez5zaMwNrf9nWKA7L06w9w/xo7uelm2/wD8Fxtv3+H8Bq5Z5v/8V+A7wBHAvgztalk2/wH0M5k9+2H5pbZ9Lf8DGdozfBX6P9gkSi9TvBINr+9P/5v7Hcu53xvrnaXc9nex+/QgPSVKXl54kSV0GhSSpy6CQJHUZFJKkLoNCktRlUEiSugwKSVLX/wdI0H3Ns87TOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "char_lens = [len(i) for i in train_df['text']]\n",
    "\n",
    "plt.hist(char_lens, bins=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b485243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 需要大概cover 95%的 data \n",
    "character_len = int(np.percentile(char_lens, 95))\n",
    "character_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c6ab01",
   "metadata": {},
   "source": [
    "## Create caracter level vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fae39fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyz0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all keyboard characters for char-level embedding\n",
    "import string\n",
    "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
    "alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b48125b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orignal Text:\n",
      "E x e r c i s e   d e c r e a s e d   c r a v i n g   r e l a t i v e   t o   b a s e l i n e   f o r   c r a v i n g   b a s e d   o n   b o t h   t h e   p l e a s u r a b l e   c o n s e q u e n c e s   o f   s m o k i n g   (   D   =   - @   o n   a   @ - p o i n t   v i s u a l   a n a l o g   s c a l e   )   a n d   a n t i c i p a t e d   r e l i e f   f r o m   w i t h d r a w a l   (   D   =   - @   )   ,   w h e r e a s   i n a c t i v i t y   i n c r e a s e d   b o t h   c o m p o n e n t s   o f   c r a v i n g   (   D s   =   @   a n d   @   )   . \n",
      "\n",
      "\n",
      "Length of text: 569\n",
      "\n",
      "Vectorized chars:\n",
      "[[ 2 24  2  8 11  4  9  2 10  2 11  8  2  5  9  2 10 11  8  5 21  4  6 18\n",
      "   8  2 12  5  3  4 21  2  3  7 22  5  9  2 12  4  6  2 17  7  8 11  8  5\n",
      "  21  4  6 18 22  5  9  2 10  7  6 22  7  3 13  3 13  2 14 12  2  5  9 16\n",
      "   8  5 22 12  2 11  7  6  9  2 26 16  2  6 11  2  9  7 17  9 15  7 23  4\n",
      "   6 18 10  7  6  5 14  7  4  6  3 21  4  9 16  5 12  5  6  5 12  7 18  9\n",
      "  11  5 12  2  5  6 10  5  6  3  4 11  4 14  5  3  2 10  8  2 12  4  2 17\n",
      "  17  8  7 15 20  4  3 13 10  8  5 20  5 12 10 20 13  2  8  2  5  9  4  6\n",
      "   5 11  3  4 21  4  3 19  4  6 11  8  2  5  9  2 10 22  7  3 13 11  7 15\n",
      "  14  7  6  2  6  3  9  7 17 11  8  5 21  4  6 18 10  9  5  6 10  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]]\n",
      "\n",
      "Length of vectorized chars: 291\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_tokens = len(alphabet)\n",
    "train_sentences = train_df['text']\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "character_vectorizer = TextVectorization(max_tokens=max_tokens, \n",
    "                                    output_sequence_length=character_len)\n",
    "\n",
    "# Adapt text vectorizer to training sentences\n",
    "character_vectorizer.adapt(train_chars)\n",
    "\n",
    "# Test out text vectorizer\n",
    "import random\n",
    "target_sentence = random.choice(train_chars)\n",
    "print(f\"Orignal Text:\\n{target_sentence}\")\n",
    "print(f\"\\nLength of text: {len(target_sentence)}\")\n",
    "print(f'\\nVectorized chars:\\n{character_vectorizer([target_sentence])}')\n",
    "print(f\"\\nLength of vectorized chars: {len(character_vectorizer([target_sentence])[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "91343e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>research num</th>\n",
       "      <th>subject</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "      <th>sentence_lens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122524</th>\n",
       "      <td>###25502343\\n</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>Patients in the antegrade group achieved bette...</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         research num  subject  \\\n",
       "122524  ###25502343\\n  RESULTS   \n",
       "\n",
       "                                                     text  line_number  \\\n",
       "122524  Patients in the antegrade group achieved bette...            8   \n",
       "\n",
       "        total_lines  sentence_lens  \n",
       "122524           17            296  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#找最长的这句话 \n",
    "train_df[train_df['sentence_lens'] == train_df['sentence_lens'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f9d36b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 291), dtype=int64, numpy=\n",
       "array([[14,  5,  3,  4,  2,  6,  3,  9,  4,  6,  3, 13,  2,  5,  6,  3,\n",
       "         2, 18,  8,  5, 10,  2, 18,  8,  7, 16, 14,  5, 11, 13,  4,  2,\n",
       "        21,  2, 10, 22,  2,  3,  3,  2,  8,  7, 16,  3, 11,  7, 15,  2,\n",
       "         9,  3, 13,  5,  6, 14,  5,  3,  4,  2,  6,  3,  9,  4,  6,  3,\n",
       "        13,  2,  8,  2,  3,  8,  7, 18,  8,  5, 10,  2, 18,  8,  7, 16,\n",
       "        14, 17,  7,  8,  5, 12, 12, 11, 12,  4,  6,  4, 11,  5, 12, 14,\n",
       "         5,  8,  5, 15,  2,  3,  2,  8,  9,  5,  3, 15,  7,  6,  3, 13,\n",
       "         9, 14,  7,  9,  3,  7, 14,  2,  8,  5,  3,  4, 21,  2, 12, 19,\n",
       "         8,  7, 15,  5,  6,  3,  2, 18,  8,  5, 10,  2, 15,  2, 10,  4,\n",
       "         5,  6, 12,  9, 22,  8,  5,  6, 18,  2,  8,  9, 22, 21,  2,  8,\n",
       "         9, 16,  9,  8,  2,  3,  8,  7, 18,  8,  5, 10,  2, 12,  9, 22,\n",
       "         8,  5,  6, 18,  2,  8,  9, 22, 10,  4, 17, 17,  2,  8,  2,  6,\n",
       "        11,  2,  7, 17, 15,  2, 10,  4,  5,  6,  9, 14, 21,  5,  9,  5,\n",
       "         6,  3,  2, 18,  8,  5, 10,  2, 15,  2, 10,  4,  5,  6,  7, 17,\n",
       "        12,  9, 22,  8,  5,  6, 18,  2,  8,  9, 22, 21,  2,  8,  9, 16,\n",
       "         9,  8,  2,  3,  8,  7, 18,  8,  5, 10,  2, 12,  9, 22,  8,  5,\n",
       "         6, 18,  2,  8,  9, 22, 10,  4, 17, 17,  2,  8,  2,  6, 11,  2,\n",
       "         7, 17, 15,  2, 10,  4,  5,  6,  9, 14, 18,  8,  4, 14,  9,  3,\n",
       "         8,  2,  6]], dtype=int64)>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 291个 caracter 长度全部装满了\n",
    "character_vectorizer([train_chars[122524]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4d6c87b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 291), dtype=int64, numpy=\n",
       "array([[15, 19,  6,  5, 15,  2,  4,  9, 23,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0]], dtype=int64)>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以看出所有的 标点符号和 空格是不算的 是不会被 vectorized 的\n",
    "character_vectorizer(['m y n a m e i s k ! @'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5110b659",
   "metadata": {},
   "source": [
    "## Find out the most characters level vector in the house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d9cbe011",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different characters in character vocab: 28\n",
      "5 most common characters: ['', '[UNK]', 'e', 't', 'i']\n",
      "5 least common characters: ['k', 'x', 'z', 'q', 'j']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'e',\n",
       " 't',\n",
       " 'i',\n",
       " 'a',\n",
       " 'n',\n",
       " 'o',\n",
       " 'r',\n",
       " 's',\n",
       " 'd',\n",
       " 'c',\n",
       " 'l',\n",
       " 'h',\n",
       " 'p',\n",
       " 'm',\n",
       " 'u',\n",
       " 'f',\n",
       " 'g',\n",
       " 'y',\n",
       " 'w',\n",
       " 'v',\n",
       " 'b',\n",
       " 'k',\n",
       " 'x',\n",
       " 'z',\n",
       " 'q',\n",
       " 'j']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 空格和 unk都算是 characters， 可以看出 特殊字符那些都被移走了 \n",
    "char_vocab = character_vectorizer.get_vocabulary()\n",
    "\n",
    "print(f\"Number of different characters in character vocab: {len(char_vocab)}\")\n",
    "print(f\"5 most common characters: {char_vocab[:5]}\")\n",
    "print(f\"5 least common characters: {char_vocab[-5:]}\")\n",
    "char_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcc312d",
   "metadata": {},
   "source": [
    "## Find out the most characters level vector with standardize on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ec876990",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different characters in character vocab: 68\n",
      "5 most common characters: ['', '[UNK]', 'e', 't', 'i']\n",
      "5 least common characters: ['+', 'X', '`', '>', 'J']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'e',\n",
       " 't',\n",
       " 'i',\n",
       " 'a',\n",
       " 'n',\n",
       " 'o',\n",
       " 'r',\n",
       " 's',\n",
       " 'd',\n",
       " 'l',\n",
       " 'c',\n",
       " 'h',\n",
       " 'p',\n",
       " 'm',\n",
       " 'u',\n",
       " 'f',\n",
       " 'g',\n",
       " '@',\n",
       " 'y',\n",
       " 'w',\n",
       " 'v',\n",
       " 'b',\n",
       " ',',\n",
       " '.',\n",
       " '-',\n",
       " ')',\n",
       " '(',\n",
       " 'T',\n",
       " 'S',\n",
       " 'C',\n",
       " 'P',\n",
       " 'A',\n",
       " 'I',\n",
       " 'k',\n",
       " 'x',\n",
       " 'R',\n",
       " '%',\n",
       " 'B',\n",
       " 'D',\n",
       " 'L',\n",
       " 'M',\n",
       " 'z',\n",
       " '=',\n",
       " 'E',\n",
       " 'H',\n",
       " 'F',\n",
       " 'N',\n",
       " 'O',\n",
       " '/',\n",
       " ';',\n",
       " 'q',\n",
       " 'V',\n",
       " 'G',\n",
       " 'j',\n",
       " 'W',\n",
       " ':',\n",
       " '<',\n",
       " 'U',\n",
       " \"'\",\n",
       " 'Q',\n",
       " 'K',\n",
       " '+',\n",
       " 'X',\n",
       " '`',\n",
       " '>',\n",
       " 'J']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 意思是当  standardize = None 这些特殊字符标点符号都要算进去了 成为一个 vector , 这样算下来的 character就有68个\n",
    "\n",
    "max_tokens = len(alphabet)\n",
    "train_sentences = train_df['text']\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "character_vectorizer_standardize_none = TextVectorization(max_tokens=max_tokens, \n",
    "                                         output_sequence_length=character_len,\n",
    "                                         #standardize=\"lower_and_strip_punctuation\" # default\n",
    "                                         standardize = None)\n",
    "\n",
    "\n",
    "character_vectorizer_standardize_none.adapt(train_chars)\n",
    "\n",
    "char_vocab_standardize_none = character_vectorizer_standardize_none.get_vocabulary()\n",
    "print(f\"Number of different characters in character vocab: {len(char_vocab_standardize_none)}\")\n",
    "print(f\"5 most common characters: {char_vocab_standardize_none[:5]}\")\n",
    "print(f\"5 least common characters: {char_vocab_standardize_none[-5:]}\")\n",
    "char_vocab_standardize_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7a7559fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 291), dtype=int64, numpy=\n",
       "array([[15, 20,  6,  5, 15,  2,  4,  9, 35,  1, 19,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0]], dtype=int64)>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这个时候后面的2个标点符号就被 vectorized 的\n",
    "character_vectorizer_standardize_none(['m y n a m e i s k ! @'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d41e9",
   "metadata": {},
   "source": [
    "## Create a character-level embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1b2cf78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charified text (before vectorization and embedding):\n",
      "h a h a h a h a\n",
      "\n",
      "Vectorized chars:\n",
      "[[13  5 13  5 13  5 13  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]]\n",
      "\n",
      "len of Vectorized chars: \n",
      "(1, 291)\n",
      "\n",
      "Embedded chars (after vectorization and embedding):\n",
      "[[[-0.02650429  0.03370489  0.00134554 ...  0.02086886 -0.03211442\n",
      "   -0.02746301]\n",
      "  [-0.00648522 -0.01911149  0.0338296  ...  0.00399175  0.01162886\n",
      "    0.00567064]\n",
      "  [-0.02650429  0.03370489  0.00134554 ...  0.02086886 -0.03211442\n",
      "   -0.02746301]\n",
      "  ...\n",
      "  [ 0.02890703 -0.00355076  0.04599346 ...  0.01781234  0.03837645\n",
      "    0.01678361]\n",
      "  [ 0.02890703 -0.00355076  0.04599346 ...  0.01781234  0.03837645\n",
      "    0.01678361]\n",
      "  [ 0.02890703 -0.00355076  0.04599346 ...  0.01781234  0.03837645\n",
      "    0.01678361]]]\n",
      "\n",
      "Character embedding shape: (1, 291, 25)\n"
     ]
    }
   ],
   "source": [
    "# Create char embedding layer\n",
    "char_embed = tf.keras.layers.Embedding(input_dim=len(char_vocab), \n",
    "                              output_dim=25, # embedding dimension of each character (same as Figure 1 in https://arxiv.org/pdf/1612.05251.pdf)\n",
    "                              mask_zero=True, # don't use masks (this messes up model_5 if set to True)\n",
    "                              name=\"char_embed\")\n",
    "\n",
    "# Test out character embedding layer\n",
    "example = random.choice(train_chars)\n",
    "example = 'h a h a h a h a'\n",
    "print(f\"Charified text (before vectorization and embedding):\\n{example}\\n\")\n",
    "\n",
    "# 变成vector的效果\n",
    "print(f'Vectorized chars:\\n{character_vectorizer([example])}\\n')\n",
    "print(f'len of Vectorized chars: \\n{character_vectorizer([example]).shape}\\n')\n",
    "# 从 vector 又加上一层 embdedding\n",
    "char_embed_example = char_embed(character_vectorizer([example]))\n",
    "print(f\"Embedded chars (after vectorization and embedding):\\n{char_embed_example}\\n\")\n",
    "print(f\"Character embedding shape: {char_embed_example.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f3071e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd7dc0c",
   "metadata": {},
   "source": [
    "## Build a Conv1D model to fit on character embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f249bee",
   "metadata": {},
   "source": [
    "### GlobalAveragePooling1D -> 0.46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "68f72650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5627\n",
      "Epoch 1/3\n",
      "562/562 [==============================] - 5s 7ms/step - loss: 1.4636 - accuracy: 0.3473 - val_loss: 1.4104 - val_accuracy: 0.3983\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 4s 7ms/step - loss: 1.3687 - accuracy: 0.4242 - val_loss: 1.3354 - val_accuracy: 0.4425\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 4s 7ms/step - loss: 1.3273 - accuracy: 0.4518 - val_loss: 1.3201 - val_accuracy: 0.4365\n",
      "Model: \"model_3_conv1D_char_embedding\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_17 (InputLayer)       [(None,)]                 0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 291)              0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " char_embed (Embedding)      (None, 291, 25)           700       \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           (None, 291, 64)           8064      \n",
      "                                                                 \n",
      " global_average_pooling1d_9   (None, 64)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,089\n",
      "Trainable params: 9,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create char datasets\n",
    "train_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, pd.get_dummies(train_df.subject))).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, pd.get_dummies(val_df.subject))).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_char_dataset = tf.data.Dataset.from_tensor_slices((test_chars, pd.get_dummies(test_df.subject))).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "print(len(train_char_dataset))\n",
    "# Create model\n",
    "inputs = tf.keras.layers.Input(shape=[], dtype= 'string')\n",
    "char_vectorizer_layer = character_vectorizer(inputs)\n",
    "char_embedding_layer= char_embed(char_vectorizer_layer)\n",
    "\n",
    "x = tf.keras.layers.Conv1D(64,kernel_size = 5, padding = 'same', activation = 'relu')(char_embedding_layer)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(5, activation = 'softmax')(x)\n",
    "\n",
    "model_3 = tf.keras.Model(inputs=inputs,\n",
    "                         outputs=outputs,\n",
    "                         name=\"model_3_conv1D_char_embedding\")\n",
    "\n",
    "model_3.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "# Fit the model on chars only\n",
    "model_3_history = model_3.fit(train_char_dataset,\n",
    "                              steps_per_epoch=int(0.1 * len(train_char_dataset)),\n",
    "                              epochs=3,\n",
    "                              validation_data=val_char_dataset,\n",
    "                              validation_steps=int(0.1 * len(val_char_dataset)),\n",
    "                             )\n",
    "\n",
    "\n",
    "\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dca0431",
   "metadata": {},
   "source": [
    "### GlobalMaxPooling1D -> 0.60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "50a06acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 5s 8ms/step - loss: 1.2498 - accuracy: 0.4869 - val_loss: 1.1281 - val_accuracy: 0.5293\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 4s 8ms/step - loss: 1.0974 - accuracy: 0.5530 - val_loss: 1.0453 - val_accuracy: 0.5745\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 4s 8ms/step - loss: 1.0278 - accuracy: 0.5904 - val_loss: 0.9826 - val_accuracy: 0.6057\n",
      "Model: \"model_3_conv1D_char_embedding\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None,)]                 0         \n",
      "                                                                 \n",
      " text_vectorization_11 (Text  (None, 291)              0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " char_embed (Embedding)      (None, 291, 25)           700       \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 291, 64)           8064      \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 64)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,089\n",
      "Trainable params: 9,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create char datasets\n",
    "train_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, pd.get_dummies(train_df.subject))).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, pd.get_dummies(val_df.subject))).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_char_dataset = tf.data.Dataset.from_tensor_slices((test_chars, pd.get_dummies(test_df.subject))).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create model\n",
    "inputs = tf.keras.layers.Input(shape=[], dtype= 'string')\n",
    "char_vectorizer_layer = character_vectorizer(inputs)\n",
    "char_embedding_layer= char_embed(char_vectorizer_layer)\n",
    "\n",
    "x = tf.keras.layers.Conv1D(64,kernel_size = 5, padding = 'same', activation = 'relu')(char_embedding_layer)\n",
    "x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(5, activation = 'softmax')(x)\n",
    "\n",
    "model_3 = tf.keras.Model(inputs=inputs,\n",
    "                         outputs=outputs,\n",
    "                         name=\"model_3_conv1D_char_embedding\")\n",
    "\n",
    "model_3.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "# Fit the model on chars only\n",
    "model_3_history = model_3.fit(train_char_dataset,\n",
    "                              steps_per_epoch=int(0.1 * len(train_char_dataset)),\n",
    "                              epochs=3,\n",
    "                              validation_data=val_char_dataset,\n",
    "                              validation_steps=int(0.1 * len(val_char_dataset)),\n",
    "                              batch_size = 32)\n",
    "\n",
    "\n",
    "\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ac73335a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30212, array([3, 3, 3, ..., 4, 2, 1], dtype=int64))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model_3.predict(val_char_dataset)\n",
    "len(pred),pred.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a103b666",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980eab88",
   "metadata": {},
   "source": [
    "# Model 4: Combining pretrained token embeddings + character embeddings (hybrid embedding layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f274b385",
   "metadata": {},
   "source": [
    "## Build the multi inputs model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8e2732c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: (((None,), (None,)), (None, 5)), types: ((tf.string, tf.string), tf.uint8)> <PrefetchDataset shapes: (((None,), (None,)), (None, 5)), types: ((tf.string, tf.string), tf.uint8)>\n",
      "Epoch 1/3\n",
      "562/562 [==============================] - 108s 184ms/step - loss: 0.9934 - accuracy: 0.6000 - val_loss: 0.7907 - val_accuracy: 0.6971\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 99s 176ms/step - loss: 0.8110 - accuracy: 0.6838 - val_loss: 0.7269 - val_accuracy: 0.7184\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 88s 157ms/step - loss: 0.7930 - accuracy: 0.6978 - val_loss: 0.7079 - val_accuracy: 0.7317\n",
      "Model: \"model_4_token_and_char_embeddings\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " char_input (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " token_input (InputLayer)       [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " text_vectorization_1 (TextVect  (None, 291)         0           ['char_input[0][0]']             \n",
      " orization)                                                                                       \n",
      "                                                                                                  \n",
      " universal_sentence_encoder (Ke  (None, 512)         256797824   ['token_input[0][0]']            \n",
      " rasLayer)                                                                                        \n",
      "                                                                                                  \n",
      " char_embed (Embedding)         (None, 291, 25)      700         ['text_vectorization_1[6][0]']   \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 128)          65664       ['universal_sentence_encoder[4][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " bidirectional_5 (Bidirectional  (None, 50)          10200       ['char_embed[6][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " token_char_hybrid (Concatenate  (None, 178)         0           ['dense_29[0][0]',               \n",
      " )                                                                'bidirectional_5[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 178)          0           ['token_char_hybrid[0][0]']      \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 128)          22912       ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 128)          0           ['dense_30[0][0]']               \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 5)            645         ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 256,897,945\n",
      "Trainable params: 100,121\n",
      "Non-trainable params: 256,797,824\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "'''#1 Create a token level embedding (tensorflow hub layer)'''\n",
    "token_inputs = layers.Input(shape=[], dtype=tf.string, name=\"token_input\")\n",
    "token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
    "token_output = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
    "token_model = tf.keras.Model(inputs=token_inputs,\n",
    "                             outputs=token_output)\n",
    "\n",
    "\n",
    "'''#2 Create a character level model (character level embedding & )'''\n",
    "char_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"char_input\")\n",
    "char_vectors = character_vectorizer(char_inputs)\n",
    "char_embeddings = char_embed(char_vectors)\n",
    "\n",
    "# copy 论文上的model - bi-LSTM shown in Figure 1 of https://arxiv.org/pdf/1612.05251.pdf\n",
    "char_bi_lstm = layers.Bidirectional(layers.LSTM(25))(char_embeddings) \n",
    "char_model = tf.keras.Model(inputs=char_inputs,\n",
    "                            outputs=char_bi_lstm)\n",
    "\n",
    "\n",
    "\n",
    "'''#3 Create tf concatenate layer (combine 1 & 2 token - embedding & character embedding)'''\n",
    "token_char_concat = layers.Concatenate(name=\"token_char_hybrid\")([token_model.output, \n",
    "                                                                  char_model.output])\n",
    "\n",
    "\n",
    "\n",
    "'''#4 Create output layer with dropout '''\n",
    "combined_dropout = layers.Dropout(0.5)(token_char_concat)\n",
    "combined_dense = layers.Dense(128, activation=\"relu\")(combined_dropout) \n",
    "final_dropout = layers.Dropout(0.5)(combined_dense)\n",
    "output_layer = layers.Dense(num_classes, activation=\"softmax\")(final_dropout)\n",
    "\n",
    "\n",
    "\n",
    "'''#5 Construct the model'''\n",
    "model_4 = tf.keras.Model(inputs=[token_model.input, char_model.input],\n",
    "                         outputs=output_layer,\n",
    "                         name=\"model_4_token_and_char_embeddings\")\n",
    "\n",
    "\n",
    "'''#6 Compile the model'''\n",
    "model_4.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(), \n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''#7 Make the model datasets (token + characters) & label'''\n",
    "# Combine chars and tokens into a dataset 前面是 token level， 后面是 character level 不要放错顺序\n",
    "train_char_token_data = tf.data.Dataset.from_tensor_slices((train_df['text'], train_chars)) \n",
    "train_char_token_labels = tf.data.Dataset.from_tensor_slices(pd.get_dummies(train_df.subject)) \n",
    "train_char_token_dataset = tf.data.Dataset.zip((train_char_token_data, train_char_token_labels)) \n",
    "\n",
    "# Prefetch and batch train data\n",
    "train_char_token_dataset = train_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) \n",
    "\n",
    "# Repeat same steps validation data\n",
    "val_char_token_data = tf.data.Dataset.from_tensor_slices((val_df['text'], val_chars))\n",
    "val_char_token_labels = tf.data.Dataset.from_tensor_slices(pd.get_dummies(val_df.subject))\n",
    "val_char_token_dataset = tf.data.Dataset.zip((val_char_token_data, val_char_token_labels))\n",
    "val_char_token_dataset = val_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# Check out training char and token embedding dataset\n",
    "print(train_char_token_dataset, val_char_token_dataset)\n",
    "\n",
    "'''#8 Fit the model'''\n",
    "# Fit the model on tokens and chars\n",
    "model_4_history = model_4.fit(train_char_token_dataset,\n",
    "                              steps_per_epoch=int(0.1 * len(train_char_token_dataset)),\n",
    "                              epochs=3,\n",
    "                              validation_data=val_char_token_dataset,\n",
    "                              validation_steps=int(0.1 * len(val_char_token_dataset)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# summary of the model \n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18815b49",
   "metadata": {},
   "source": [
    "## accuracy -> 0.73"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc72ae8",
   "metadata": {},
   "source": [
    "# Model 5: Transfer Learning with pretrained token embeddings + character embeddings + positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "68ee88b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>research num</th>\n",
       "      <th>subject</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>###24293578\\n</td>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>To investigate the efficacy of @ weeks of dail...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>###24293578\\n</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>A total of @ patients with primary knee OA wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>###24293578\\n</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>Outcome measures included pain reduction and i...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>###24293578\\n</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>Pain was assessed using the visual analog pain...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>###24293578\\n</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>Secondary outcome measures included the Wester...</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>###24293578\\n</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>Serum levels of interleukin @ ( IL-@ ) , IL-@ ...</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>###24293578\\n</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>There was a clinically relevant reduction in t...</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>###24293578\\n</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>The mean difference between treatment arms ( @...</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>###24293578\\n</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>Further , there was a clinically relevant redu...</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>###24293578\\n</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>These differences remained significant at @ we...</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>###24293578\\n</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>The Outcome Measures in Rheumatology Clinical ...</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>###24293578\\n</td>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>Low-dose oral prednisolone had both a short-te...</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>###24854809\\n</td>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>Emotional eating is associated with overeating...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     research num      subject  \\\n",
       "0   ###24293578\\n    OBJECTIVE   \n",
       "1   ###24293578\\n      METHODS   \n",
       "2   ###24293578\\n      METHODS   \n",
       "3   ###24293578\\n      METHODS   \n",
       "4   ###24293578\\n      METHODS   \n",
       "5   ###24293578\\n      METHODS   \n",
       "6   ###24293578\\n      RESULTS   \n",
       "7   ###24293578\\n      RESULTS   \n",
       "8   ###24293578\\n      RESULTS   \n",
       "9   ###24293578\\n      RESULTS   \n",
       "10  ###24293578\\n      RESULTS   \n",
       "11  ###24293578\\n  CONCLUSIONS   \n",
       "12  ###24854809\\n   BACKGROUND   \n",
       "\n",
       "                                                 text  line_number  \\\n",
       "0   To investigate the efficacy of @ weeks of dail...            0   \n",
       "1   A total of @ patients with primary knee OA wer...            1   \n",
       "2   Outcome measures included pain reduction and i...            2   \n",
       "3   Pain was assessed using the visual analog pain...            3   \n",
       "4   Secondary outcome measures included the Wester...            4   \n",
       "5   Serum levels of interleukin @ ( IL-@ ) , IL-@ ...            5   \n",
       "6   There was a clinically relevant reduction in t...            6   \n",
       "7   The mean difference between treatment arms ( @...            7   \n",
       "8   Further , there was a clinically relevant redu...            8   \n",
       "9   These differences remained significant at @ we...            9   \n",
       "10  The Outcome Measures in Rheumatology Clinical ...           10   \n",
       "11  Low-dose oral prednisolone had both a short-te...           11   \n",
       "12  Emotional eating is associated with overeating...            0   \n",
       "\n",
       "    total_lines  \n",
       "0            12  \n",
       "1            12  \n",
       "2            12  \n",
       "3            12  \n",
       "4            12  \n",
       "5            12  \n",
       "6            12  \n",
       "7            12  \n",
       "8            12  \n",
       "9            12  \n",
       "10           12  \n",
       "11           12  \n",
       "12           11  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bd5a44",
   "metadata": {},
   "source": [
    "## Every sentence has a subject (每一个句子都是有主题的)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7a9e8e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .\\n'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#这里是one hot line number 其实还可以 onehot subject 节约metrix\n",
    "train_df['text'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbab5992",
   "metadata": {},
   "source": [
    "## Dummy the line numbers per research paper (Tensorflow one hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "db37f15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     15000\n",
       "1     15000\n",
       "2     15000\n",
       "3     15000\n",
       "4     14992\n",
       "      ...  \n",
       "26        7\n",
       "27        4\n",
       "28        3\n",
       "29        1\n",
       "30        1\n",
       "Name: line_number, Length: 31, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['line_number'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8a66c888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cut off at 15 , 我们可以保留大部分的信息\n",
    "train_df.line_number.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2a844419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180035</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180036</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180037</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180038</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180039</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180040 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  \\\n",
       "0        1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "1        0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "2        0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "3        0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "4        0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "...     ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..   \n",
       "180035   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   \n",
       "180036   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   \n",
       "180037   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   \n",
       "180038   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   \n",
       "180039   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   \n",
       "\n",
       "        17  18  19  20  21  22  23  24  25  26  27  28  29  30  \n",
       "0        0   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
       "1        0   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
       "2        0   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
       "3        0   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
       "4        0   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
       "...     ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  \n",
       "180035   0   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
       "180036   0   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
       "180037   0   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
       "180038   0   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
       "180039   0   0   0   0   0   0   0   0   0   0   0   0   0   0  \n",
       "\n",
       "[180040 rows x 31 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_line_num = pd.get_dummies(train_df.line_number,)\n",
    "dummy_line_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ecb390ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(180040, 15), dtype=float32, numpy=\n",
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#这里是one hot line number 其实还可以 onehot subject 节约metrix\n",
    "\n",
    "dummy_line_num_train = tf.one_hot(train_df[\"line_number\"].to_numpy(), depth=15)\n",
    "dummy_line_num_val = tf.one_hot(val_df[\"line_number\"].to_numpy(), depth=15)\n",
    "dummy_line_num_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49677164",
   "metadata": {},
   "source": [
    "## Dummy the total lines per research paper (Tensorflow one hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "30883326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<AxesSubplot:>, 19.0)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 选20就可以了\n",
    "train_df['total_lines'].hist(),np.percentile(train_df.total_lines,95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5f165b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_total_line_train = tf.one_hot(train_df[\"total_lines\"].to_numpy(), depth=20)\n",
    "dummy_total_line_vali = tf.one_hot(val_df[\"total_lines\"].to_numpy(), depth=20)\n",
    "dummy_total_line_test = tf.one_hot(test_df[\"total_lines\"].to_numpy(), depth=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79157db2",
   "metadata": {},
   "source": [
    "## Make the tribrid model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e6a381e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Token inputs\n",
    "token_inputs = layers.Input(shape=[], dtype=\"string\", name=\"token_inputs\")\n",
    "token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
    "token_outputs = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
    "token_model = tf.keras.Model(inputs=token_inputs,\n",
    "                             outputs=token_outputs)\n",
    "\n",
    "# 2. Char inputs\n",
    "char_inputs = layers.Input(shape=(1,), dtype=\"string\", name=\"char_inputs\")\n",
    "char_vectors = character_vectorizer(char_inputs)\n",
    "char_embeddings = char_embed(char_vectors)\n",
    "char_bi_lstm = layers.Bidirectional(layers.LSTM(32))(char_embeddings)\n",
    "char_model = tf.keras.Model(inputs=char_inputs,\n",
    "                            outputs=char_bi_lstm)\n",
    "\n",
    "# 3. Line numbers inputs\n",
    "line_number_inputs = layers.Input(shape=(15,), dtype=tf.int32, name=\"line_number_input\")\n",
    "x = layers.Dense(32, activation=\"relu\")(line_number_inputs)\n",
    "line_number_model = tf.keras.Model(inputs=line_number_inputs,\n",
    "                                   outputs=x)\n",
    "\n",
    "# 4. Total lines inputs\n",
    "total_lines_inputs = layers.Input(shape=(20,), dtype=tf.int32, name=\"total_lines_input\")\n",
    "y = layers.Dense(32, activation=\"relu\")(total_lines_inputs)\n",
    "total_line_model = tf.keras.Model(inputs=total_lines_inputs,\n",
    "                                  outputs=y)\n",
    "\n",
    "# 5. Combine token and char embeddings into a hybrid embedding\n",
    "combined_embeddings = layers.Concatenate(name=\"token_char_hybrid_embedding\")([token_model.output, \n",
    "                                                                              char_model.output])\n",
    "z = layers.Dense(256, activation=\"relu\")(combined_embeddings)\n",
    "z = layers.Dropout(0.5)(z)\n",
    "\n",
    "# 6. Combine positional embeddings with combined token and char embeddings into a tribrid embedding\n",
    "z = layers.Concatenate(name=\"token_char_positional_embedding\")([line_number_model.output,\n",
    "                                                                total_line_model.output,\n",
    "                                                                z])\n",
    "\n",
    "# 7. Create output layer\n",
    "output_layer = layers.Dense(5, activation=\"softmax\", name=\"output_layer\")(z)\n",
    "\n",
    "# 8. Put together model\n",
    "model_5 = tf.keras.Model(inputs=[line_number_model.input,\n",
    "                                 total_line_model.input,\n",
    "                                 token_model.input, \n",
    "                                 char_model.input],\n",
    "                         outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b1a6fd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_27\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " char_inputs (InputLayer)       [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " token_inputs (InputLayer)      [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " text_vectorization_1 (TextVect  (None, 291)         0           ['char_inputs[0][0]']            \n",
      " orization)                                                                                       \n",
      "                                                                                                  \n",
      " universal_sentence_encoder (Ke  (None, 512)         256797824   ['token_inputs[0][0]']           \n",
      " rasLayer)                                                                                        \n",
      "                                                                                                  \n",
      " char_embed (Embedding)         (None, 291, 25)      700         ['text_vectorization_1[7][0]']   \n",
      "                                                                                                  \n",
      " dense_34 (Dense)               (None, 128)          65664       ['universal_sentence_encoder[7][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " bidirectional_6 (Bidirectional  (None, 64)          14848       ['char_embed[7][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " token_char_hybrid_embedding (C  (None, 192)         0           ['dense_34[0][0]',               \n",
      " oncatenate)                                                      'bidirectional_6[0][0]']        \n",
      "                                                                                                  \n",
      " line_number_input (InputLayer)  [(None, 15)]        0           []                               \n",
      "                                                                                                  \n",
      " total_lines_input (InputLayer)  [(None, 20)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_37 (Dense)               (None, 256)          49408       ['token_char_hybrid_embedding[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 32)           512         ['line_number_input[0][0]']      \n",
      "                                                                                                  \n",
      " dense_36 (Dense)               (None, 32)           672         ['total_lines_input[0][0]']      \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 256)          0           ['dense_37[0][0]']               \n",
      "                                                                                                  \n",
      " token_char_positional_embeddin  (None, 320)         0           ['dense_35[0][0]',               \n",
      " g (Concatenate)                                                  'dense_36[0][0]',               \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " output_layer (Dense)           (None, 5)            1605        ['token_char_positional_embedding\n",
      "                                                                 [0][0]']                         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 256,931,233\n",
      "Trainable params: 133,409\n",
      "Non-trainable params: 256,797,824\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666ea7c7",
   "metadata": {},
   "source": [
    "## Complie the tribird model with label_smoothing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e50dc433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile token, char, positional embedding model\n",
    "model_5.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67d833f",
   "metadata": {},
   "source": [
    "## Create the data input for the tribird model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0dc89e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (((None, 15), (None, 20), (None,), (None,)), (None, 5)), types: ((tf.float32, tf.float32, tf.string, tf.string), tf.uint8)>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里需要注意的是 input order要和 model inputs顺序一样\n",
    "'''\n",
    "1. line_number_model.input,\n",
    "2. total_line_model.input,\n",
    "3. token_model.input, \n",
    "4. char_model.input,\n",
    "'''       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create training datasets (all four kinds of inputs)\n",
    "train_position_charactor_token_dataset = tf.data.Dataset.from_tensor_slices((dummy_line_num_train, # line numbers\n",
    "                                                                dummy_total_line_train, # total lines\n",
    "                                                                train_sentences, # train tokens\n",
    "                                                                train_chars)) # train chars\n",
    "\n",
    "# create traning label\n",
    "train_position_charactor_token_labels = tf.data.Dataset.from_tensor_slices(pd.get_dummies(train_df.subject)) # train labels\n",
    "\n",
    "\n",
    "# zip train dataset & label together \n",
    "train_pos_char_token_dataset = tf.data.Dataset.zip((train_position_charactor_token_dataset, train_position_charactor_token_labels)) \n",
    "\n",
    "# batch and prefetch as much as possible data\n",
    "train_pos_char_token_dataset = train_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) \n",
    "train_pos_char_token_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "745bf03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (((None, 15), (None, 20), (None,), (None,)), (None, 5)), types: ((tf.float32, tf.float32, tf.string, tf.string), tf.uint8)>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation dataset\n",
    "val_pos_char_token_data = tf.data.Dataset.from_tensor_slices((dummy_line_num_val,\n",
    "                                                              dummy_total_line_vali,\n",
    "                                                              val_df.text,\n",
    "                                                              val_chars))\n",
    "val_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(pd.get_dummies(val_df.subject))\n",
    "val_pos_char_token_dataset = tf.data.Dataset.zip((val_pos_char_token_data, val_pos_char_token_labels))\n",
    "val_pos_char_token_dataset = val_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) \n",
    "val_pos_char_token_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfe955b",
   "metadata": {},
   "source": [
    "## Fit the tribird model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b80abf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 91s 151ms/step - loss: 1.1004 - accuracy: 0.7172 - val_loss: 0.9830 - val_accuracy: 0.8072\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 105s 187ms/step - loss: 0.9690 - accuracy: 0.8137 - val_loss: 0.9521 - val_accuracy: 0.8278\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 104s 186ms/step - loss: 0.9518 - accuracy: 0.8223 - val_loss: 0.9398 - val_accuracy: 0.8318\n"
     ]
    }
   ],
   "source": [
    "history_model_5 = model_5.fit(train_pos_char_token_dataset,\n",
    "                              \n",
    "                              # 仅仅用了 10%的off data to training， 如果是所有的data 那么准确率会更高\n",
    "                              steps_per_epoch=int(0.1 * len(train_pos_char_token_dataset)),\n",
    "                              epochs=3,\n",
    "                              validation_data=val_pos_char_token_dataset,\n",
    "                              validation_steps=int(0.1 * len(val_pos_char_token_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ed123",
   "metadata": {},
   "source": [
    "## Accuracy -> 0.83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b762a9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 46s 48ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.50036097, 0.08651903, 0.01511671, 0.3792464 , 0.01875687],\n",
       "       [0.5509153 , 0.09670322, 0.04211105, 0.2995703 , 0.01070011],\n",
       "       [0.34833878, 0.1067544 , 0.15623432, 0.3208436 , 0.06782896],\n",
       "       ...,\n",
       "       [0.0341695 , 0.11025728, 0.04063736, 0.02579984, 0.78913605],\n",
       "       [0.02791951, 0.30305544, 0.07454214, 0.02399205, 0.5704909 ],\n",
       "       [0.3049435 , 0.50846285, 0.07934643, 0.0423336 , 0.06491362]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_pred_probs = model_5.predict(val_pos_char_token_dataset, verbose=1)\n",
    "model_5_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d30a29bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 4, 4, 1], dtype=int64)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_pred_probs.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8a41d1",
   "metadata": {},
   "source": [
    "# Save the best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d2883610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_19_layer_call_fn, lstm_cell_19_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_19_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: skimlit_tribrid_model2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: skimlit_tribrid_model2\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000001BA0E6FAE80> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000001BA0E7BEDF0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "model_5.save(\"skimlit_tribrid_model2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a743be8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"forward_lstm_6\" (type LSTM).\n\nThe mask that was passed in was tf.RaggedTensor(values=Tensor(\"Placeholder_2:0\", shape=(None,), dtype=bool), row_splits=Tensor(\"Placeholder_3:0\", shape=(None,), dtype=int64)), which cannot be applied to RaggedTensor inputs. Please make sure that there is no mask injected by upstream layers.\n\nCall arguments received:\n  • inputs=tf.RaggedTensor(values=Tensor(\"Placeholder:0\", shape=(None, 25), dtype=float32), row_splits=Tensor(\"Placeholder_1:0\", shape=(None,), dtype=int64))\n  • mask=tf.RaggedTensor(values=Tensor(\"Placeholder_2:0\", shape=(None,), dtype=bool), row_splits=Tensor(\"Placeholder_3:0\", shape=(None,), dtype=int64))\n  • training=None\n  • initial_state=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-152-52549cc8593b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbestmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skimlit_tribrid_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbestmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m_validate_args_if_ragged\u001b[1;34m(self, is_ragged_input, mask)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 901\u001b[1;33m       raise ValueError(f'The mask that was passed in was {mask}, which '\n\u001b[0m\u001b[0;32m    902\u001b[0m                        \u001b[1;34m'cannot be applied to RaggedTensor inputs. Please '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m                        \u001b[1;34m'make sure that there is no mask injected by upstream '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"forward_lstm_6\" (type LSTM).\n\nThe mask that was passed in was tf.RaggedTensor(values=Tensor(\"Placeholder_2:0\", shape=(None,), dtype=bool), row_splits=Tensor(\"Placeholder_3:0\", shape=(None,), dtype=int64)), which cannot be applied to RaggedTensor inputs. Please make sure that there is no mask injected by upstream layers.\n\nCall arguments received:\n  • inputs=tf.RaggedTensor(values=Tensor(\"Placeholder:0\", shape=(None, 25), dtype=float32), row_splits=Tensor(\"Placeholder_1:0\", shape=(None,), dtype=int64))\n  • mask=tf.RaggedTensor(values=Tensor(\"Placeholder_2:0\", shape=(None,), dtype=bool), row_splits=Tensor(\"Placeholder_3:0\", shape=(None,), dtype=int64))\n  • training=None\n  • initial_state=None"
     ]
    }
   ],
   "source": [
    "bestmodel = tf.keras.models.load_model('skimlit_tribrid_model')\n",
    "bestmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f11c52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "20+19+18+18+18+18"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "433px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
